{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Копия блокнота \"farich_read_root_example.ipynb\"",
   "provenance": [
    {
     "file_id": "11NebRxpDewALrvN8Un_jA6atrZ3L4HlN",
     "timestamp": 1652210152468
    },
    {
     "file_id": "1VFgar3L6EujFLgBfbXNjISVZZ2F2k15t",
     "timestamp": 1631255116354
    },
    {
     "file_id": "19drm9hSuUtYHZlxN3xIVb7R-UoJ0EpDJ",
     "timestamp": 1619687086093
    },
    {
     "file_id": "1AWtfk-SUO5iUQL6SPYK5X3VzsIWD7j4Q",
     "timestamp": 1618441852195
    },
    {
     "file_id": "1yEW57kkrxMiquNsmuG7Z13pyi9YkIGag",
     "timestamp": 1617528150642
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0dQArAzUdno",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898444844,
     "user_tz": -180,
     "elapsed": 404,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "outputId": "05d97e3b-58af-45c6-c325-bba6a4b5f0d4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Подключение пакетов\n",
    "import os, sys, time\n",
    "import uproot3 as uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from mpl_toolkits import mplot3d\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from numbers import Integral\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm, truncnorm, foldnorm\n",
    "from itertools import compress\n",
    "from pynverse import inversefunc\n",
    "import warnings\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "print('Uproot version:',uproot.version.version)\n",
    "print('Numpy version:', np.version.version)\n",
    "print('Pandas version:', pd.__version__)\n",
    "plt.ioff()\n",
    "\n",
    "# инициализация генератора псевдослучайных чисел\n",
    "# pandas version was 1.5.1\n",
    "\n",
    "rng = np.random.default_rng(12345)"
   ],
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uproot version: 3.14.4\n",
      "Numpy version: 1.23.4\n",
      "Pandas version: 2.0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "datadir = 'data'\n",
    "picsdir = 'pics'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LwzFMaFOPreX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898469884,
     "user_tz": -180,
     "elapsed": 263,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# отображение ключей в файле uproot в виде иерархии\n",
    "def show_uproot_tree(obj, maxkeylen=12, sep='/', indent=0) -> None:\n",
    "  width = maxkeylen+len(sep)\n",
    "  startline = False\n",
    "  if isinstance(obj, uproot.rootio.ROOTDirectory):\n",
    "    print('TFile: '+obj.name.decode('utf-8'))\n",
    "    startline = True\n",
    "    indent = 2\n",
    "  elif issubclass(type(obj), uproot.tree.TTreeMethods):\n",
    "    print('TTree: '+obj.name.decode('utf-8'))\n",
    "    startline = True\n",
    "    indent = 4\n",
    "  else:\n",
    "    if len(obj.keys()) > 0:\n",
    "      indent += width\n",
    "      s = obj.name.decode('utf-8')[:maxkeylen]\n",
    "      print(s + ' '*(maxkeylen-len(s)) + sep, end='')\n",
    "    else:\n",
    "      print(obj.name.decode('utf-8'))\n",
    "\n",
    "  if len(obj.keys()) > 0:\n",
    "    for i, key in enumerate(obj.keys()):\n",
    "      if i>0 or startline:\n",
    "        print(' '*indent, end='')\n",
    "      show_uproot_tree(obj[key], indent=indent)\n",
    "    indent -= width"
   ],
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lg70kDuWKx1d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898474427,
     "user_tz": -180,
     "elapsed": 2763,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "outputId": "d3b922b3-ad42-4ec9-e62e-f8fc027d0904",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "filepath = os.path.join(datadir, 'farichsim_1200kevt.root')\n",
    "show_uproot_tree(uproot.open(filepath))"
   ],
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFile: ./farichsim_pi-pi-_45-360deg_1200.0k_ideal_2020-12-24_rndm.root\n",
      "  TTree: info_sim\n",
      "    info_gen    /m_num_events\n",
      "                 m_z_dis\n",
      "    info_rad    /m_layers    /m_layers.first\n",
      "                              m_layers.second\n",
      "    info_pmt    /m_name\n",
      "                 m_num_side_x\n",
      "                 m_num_side_y\n",
      "                 m_gap\n",
      "                 m_size\n",
      "                 m_chip_num_size\n",
      "                 m_chip_pitch\n",
      "                 m_chip_size\n",
      "                 m_chip_offset\n",
      "                 m_focal_length\n",
      "                 m_trg_window\n",
      "                 m_origin_pos/m_origin_pos._2\n",
      "                              m_origin_pos._1\n",
      "                              m_origin_pos._0\n",
      "  TTree: raw_data\n",
      "    event       /m_id_event\n",
      "                 m_id_primary\n",
      "                 m_pos_primar/m_pos_primary._2\n",
      "                              m_pos_primary._1\n",
      "                              m_pos_primary._0\n",
      "                 m_dir_primar/m_dir_primary._2\n",
      "                              m_dir_primary._1\n",
      "                              m_dir_primary._0\n",
      "                 m_theta_primary\n",
      "                 m_phi_primary\n",
      "                 m_momentum_primary\n",
      "                 m_beta_primary\n",
      "                 m_hits      /m_hits.m_photon_id_pmt\n",
      "                              m_hits.m_photon_id_chip\n",
      "                              m_hits.m_photon_id_layer\n",
      "                              m_hits.m_photon_id_track\n",
      "                              m_hits.m_photon_id_track_parent\n",
      "                              m_hits.m_photon_id_hit\n",
      "                              m_hits.m_photon_wl\n",
      "                              m_hits.m_photon_time\n",
      "                              m_hits.m_photon_pos_exact._2\n",
      "                              m_hits.m_photon_pos_exact._1\n",
      "                              m_hits.m_photon_pos_exact._0\n",
      "                              m_hits.m_photon_pos_chip._2\n",
      "                              m_hits.m_photon_pos_chip._1\n",
      "                              m_hits.m_photon_pos_chip._0\n",
      "                              m_hits.m_photon_pos_vertex._2\n",
      "                              m_hits.m_photon_pos_vertex._1\n",
      "                              m_hits.m_photon_pos_vertex._0\n",
      "                              m_hits.m_photon_dir_vertex._2\n",
      "                              m_hits.m_photon_dir_vertex._1\n",
      "                              m_hits.m_photon_dir_vertex._0\n",
      "                              m_hits.m_hit_is_fittable\n",
      "                              m_hits.m_photon_theta.first\n",
      "                              m_hits.m_photon_theta.second\n",
      "                              m_hits.m_photon_phi.first\n",
      "                              m_hits.m_photon_phi.second\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X1oKpqDKUfrD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898477207,
     "user_tz": -180,
     "elapsed": 332,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def readInfoFromRoot(filepath, verbose: bool = False) -> pd.DataFrame:\n",
    "  '''\n",
    "  Получение информации о моделировании из ROOT-файла в виде датафрейм формой (1, N), где N - число параметров.\n",
    "  '''\n",
    "  # Названия используемых колонок данных для переименования и сохранения в data frame\n",
    "  idf_rename_map = {'m_num_events': 'nevents',  # число событий моделирования\n",
    "                    'm_z_dis': 'zdis',  # расстояние от места рождения частицы до входа в радиатор в мм\n",
    "                    'm_layers': 'nlayers',  # число слоев радиатора\n",
    "                    'm_size': 'array_size',  # размер матрицы КФУ в мм\n",
    "                    'm_gap': 'array_gap',  # зазор между матрицами КФУ в мм\n",
    "                    'm_chip_size': 'pixel_size',  # размер пикселя КФУ в мм\n",
    "                    'm_chip_pitch': 'pixel_gap',  # зазор между пикселями КФУ в мм\n",
    "                    'm_chip_num_size': 'pixel_numx',  # размер матрицы КФУ в пикселях\n",
    "                    'm_num_side_x': 'nxarrays', 'm_num_side_y': 'nyarrays',  # размер фотодетектора в матрицах КФУ по X и Y\n",
    "                    'm_focal_length': 'distance',  # расстояние от входа в радиатор до входа в фотодетектор\n",
    "                    'm_trg_window': 'trg_window_ns',  # размер временного окна в нс\n",
    "                    'W': 'W',  # толщина радиатора в мм (вычисляемая)\n",
    "                    'n_mean': 'n_mean',  # средний показатель преломления радиатора (вычисляемый)\n",
    "                    'n_max': 'n_max',  # максимальный показатель преломления радиатора (вычисляемый)\n",
    "                   }\n",
    "\n",
    "  # Открытие ROOT-файла с данными используя Uproot https://github.com/scikit-hep/uproot3\n",
    "  with uproot.open(filepath) as file:\n",
    "    idf = file[b'info_sim'].pandas.df('*', flatten=False)\n",
    "\n",
    "  # Переименование параметров\n",
    "  idf.rename(columns=idf_rename_map, inplace=True, errors='ignore')\n",
    "\n",
    "  # Получение параметров (многослойного) радиатора одинаковых для всех файлов\n",
    "  n_l = idf.at[0,'m_layers.first']  # показатели преломления слоёв\n",
    "  w_l = idf.at[0,'m_layers.second']  # толщины слоёв радиатора\n",
    "\n",
    "  W = w_l.sum()  # суммарная толщина всех слоёв\n",
    "  n_mean = n_l.mean()  # средний показатель преломления\n",
    "  n_max = n_l.max()  # максимальный показатель преломления\n",
    "\n",
    "  # Добавление вычисляемых параметров в idf\n",
    "  idf['W'] = W\n",
    "  idf['n_mean'] = n_mean\n",
    "  idf['n_max'] = n_max\n",
    "\n",
    "  # Сохранение нужных параметров\n",
    "  idf = idf[idf_rename_map.values()]\n",
    "\n",
    "  if verbose:\n",
    "    for name in idf.columns:\n",
    "      print(f'{name}: {idf.at[0, name]}')\n",
    "\n",
    "  return idf"
   ],
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y8Ku3fKp8Vxl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898480118,
     "user_tz": -180,
     "elapsed": 3,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "outputId": "3541df8f-bc75-45f9-e0a7-6d1ff9183be1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "idf = readInfoFromRoot(filepath)"
   ],
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "       nevents  zdis  nlayers  array_size  array_gap  pixel_size  pixel_gap   \nentry                                                                         \n0      1200000   1.0        4       26.68        1.0        3.16        0.2  \\\n\n       pixel_numx  nxarrays  nyarrays  distance  trg_window_ns     W  n_mean   \nentry                                                                          \n0               8        30        30     200.0           20.0  35.0  1.0454  \\\n\n       n_max  \nentry         \n0       1.05  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nevents</th>\n      <th>zdis</th>\n      <th>nlayers</th>\n      <th>array_size</th>\n      <th>array_gap</th>\n      <th>pixel_size</th>\n      <th>pixel_gap</th>\n      <th>pixel_numx</th>\n      <th>nxarrays</th>\n      <th>nyarrays</th>\n      <th>distance</th>\n      <th>trg_window_ns</th>\n      <th>W</th>\n      <th>n_mean</th>\n      <th>n_max</th>\n    </tr>\n    <tr>\n      <th>entry</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1200000</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>26.68</td>\n      <td>1.0</td>\n      <td>3.16</td>\n      <td>0.2</td>\n      <td>8</td>\n      <td>30</td>\n      <td>30</td>\n      <td>200.0</td>\n      <td>20.0</td>\n      <td>35.0</td>\n      <td>1.0454</td>\n      <td>1.05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sCexVXIHFivc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898482259,
     "user_tz": -180,
     "elapsed": 318,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def genChunkFromRoot(filepath, eventchunksize=2000, noisefreqpersqmm: float = 2e6, noiseTimeRange: float = (0, 7), shiftSignalTimes: bool = True,\n",
    "                     edfstore: pd.HDFStore = None, verbose: bool = True) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "    Генератор событий из ROOT-файла в виде датафрейма. Число событий eventchunksize, читаемых генератором за один раз, должен выбираться так,\n",
    "    чтобы все данные с учетом добавляемых шумовых срабатываний умещались в размер ОЗУ.\n",
    "\n",
    "    Параметры:\n",
    "    filepath - путь к ROOT-файлу для чтения.\n",
    "    eventchunksize - число событий, загружаемых из ROOT-файла за один вызов.\n",
    "    noisefreqpersqmm - частота темновых срабатываний на единицу активной площади фотодетектора в с^{-1}*мм^{-2}, подмешиваемых к событиям;\n",
    "                       максимальное значение параметра, которое имеет смысл рассматривать, 2e6.\n",
    "    noiseTimeRange - (start, stop) - tuple, задающий временной интервал генерации шума в наносекундах.\n",
    "    edfstore - HDF-хранилище для записи датафрейма \"edf\"; данные добавляются к уже записанным в хранилище.\n",
    "    verbose - флаг отладочной печати.\n",
    "\n",
    "    Описание условий моделирования:\n",
    "    Ось Z направлена по нормали к плоскости радиатора от радиатора к фотодетектору.\n",
    "    Оси X и Y паралельны осям симметрии матрицы фотодетектора.\n",
    "    Первичная частица (отрицательный пион) вылетает на расстоянии zdis=1 мм перед радиатором в его сторону\n",
    "    Начальное положение частицы случайно разбрасывается по X и Y в квадрате со стороной (pixel_size+pixel_gap).\n",
    "    Направление частицы случайно разбрасывается в телесном угле в пределах theta_p=[0, π/4], phi_p=[0, 2π].\n",
    "    Скорость частицы случайно и равномерно разбрасывается от 0.957 до 0.999 скорости света.\n",
    "  \"\"\"\n",
    "  global rng\n",
    "\n",
    "  # Данные о частице (для переименования и сохранения)\n",
    "  part_rename_map = {'m_hits': 'nhits',                # число срабатываний в событии\n",
    "                     'm_pos_primary._0': 'x_p',        # X-координата вылета частицы в мм\n",
    "                     'm_pos_primary._1': 'y_p',        # Y-координата вылета частицы в мм\n",
    "                     'm_pos_primary._2': 'z_p',        # Z-координата вылета частицы в мм\n",
    "                     'm_dir_primary._0': 'nx_p',       # X-компонента единичного вектора направления частицы\n",
    "                     'm_dir_primary._1': 'ny_p',       # Y-компонента единичного вектора направления частицы\n",
    "                     'm_dir_primary._2': 'nz_p',       # Z-компонента единичного вектора направления частицы\n",
    "                     'm_beta_primary': 'beta',         # скорость частицы в единицах скорости света\n",
    "                     'm_theta_primary': 'theta_p',     # полярный угол направления частицы в радианах\n",
    "                     'm_phi_primary': 'phi_p',         # азимутальный угол направления частицы в радианах\n",
    "                     'm_momentum_primary': 'momentum'  # импульс частицы в МэВ/c\n",
    "                    }\n",
    "  \n",
    "  # Наблюдаемые данные о срабатываниях (для переименования и сохранения)\n",
    "  hit_rename_map = {'m_hits.m_photon_pos_chip._0': 'x_c',  # X-координата срабатывания в мм\n",
    "                    'm_hits.m_photon_pos_chip._1': 'y_c',  # Y-координата срабатывания в мм\n",
    "                    'm_hits.m_photon_pos_chip._2': 'z_c',  # Z-координата срабатывания в мм\n",
    "                    'm_hits.m_photon_time': 't_c'          # время срабатывания в нс\n",
    "                   }\n",
    "\n",
    "  other_rename_map = {'m_id_event': '',                         #\n",
    "                      'm_id_primary': '',                       #\n",
    "                      'm_hits.m_photon_id_pmt': '',             #\n",
    "                      'm_hits.m_photon_id_chip': '',            #\n",
    "                      'm_hits.m_photon_id_layer': '',           #\n",
    "                      'm_hits.m_photon_id_track': '',           #\n",
    "                      'm_hits.m_photon_id_track_parent': '',    #\n",
    "                      'm_hits.m_photon_id_hit': '',             #\n",
    "                      'm_hits.m_photon_wl': '',                 #\n",
    "                      'm_hits.m_photon_pos_exact._0': '',       #\n",
    "                      'm_hits.m_photon_pos_exact._1': '',       #\n",
    "                      'm_hits.m_photon_pos_exact._2': '',       #\n",
    "                      'm_hits.m_photon_pos_vertex._0': '',      #\n",
    "                      'm_hits.m_photon_pos_vertex._1': '',      #\n",
    "                      'm_hits.m_photon_pos_vertex._2': '',      #\n",
    "                      'm_hits.m_photon_dir_vertex._0': '',      #\n",
    "                      'm_hits.m_photon_dir_vertex._1': '',      #\n",
    "                      'm_hits.m_photon_dir_vertex._2': '',      #\n",
    "                      'm_hits.m_hit_is_fittable': '',           #\n",
    "                      'm_hits.m_photon_theta.first': '',        #\n",
    "                      'm_hits.m_photon_theta.second': '',       #\n",
    "                      'm_hits.m_photon_phi.first': '',          #\n",
    "                      'm_hits.m_photon_phi.second': '',         #\n",
    "                      }\n",
    "\n",
    "  for key in other_rename_map:\n",
    "    other_rename_map.update({key: key})\n",
    "  # Наименования колонок для сохранения в датафрейм\n",
    "  edfcolstosave = list(part_rename_map.values()) + list(hit_rename_map.values())\n",
    "\n",
    "  # Чтение параметров моделирования\n",
    "  idf = readInfoFromRoot(filepath)\n",
    "\n",
    "  # Определения параметров фотодетектора для генерации темнового шума\n",
    "  pixel_size, pixel_gap = idf.at[0, 'pixel_size'], idf.at[0, 'pixel_gap']\n",
    "  array_size, array_gap = idf.at[0, 'array_size'], idf.at[0, 'array_gap']\n",
    "  nxpixels_arr = idf.at[0, 'pixel_numx']\n",
    "  nxpixels_tot = idf.at[0, 'nxarrays']*nxpixels_arr\n",
    "  igrid = np.arange(nxpixels_tot//2)\n",
    "  xpnts = array_gap/2 + (igrid//nxpixels_arr)*(array_size+array_gap) + (igrid%nxpixels_arr)*(pixel_size+pixel_gap) + pixel_size/2\n",
    "  xpnts = np.sort(np.append(-xpnts, xpnts)).astype('float32')\n",
    "  xgrid, ygrid = np.meshgrid(xpnts, xpnts)\n",
    "  xgrid = xgrid.reshape(xgrid.size)\n",
    "  ygrid = ygrid.reshape(ygrid.size)\n",
    " \n",
    "  def addNoise(partdf: pd.DataFrame, hitdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    ''' \n",
    "    Генерация темновых срабатываний темнового шума и добавление в датафрейм (без учета \"мёртвого\" времени пикселя).\n",
    "    partdf - датафрейм для частиц\n",
    "    hitdf - датафрейм для срабатываний\n",
    "    '''\n",
    "    assert(np.isclose(pixel_size*nxpixels_arr+pixel_gap*(nxpixels_arr-1), array_size))\n",
    "    nevents = partdf.shape[0]  # число событий\n",
    "    \n",
    "    # среднее число шумовых срабатываний на событие\n",
    "    munoise = (noiseTimeRange[1]-noiseTimeRange[0])*1e-9*noisefreqpersqmm*(pixel_size**2)*(nxpixels_tot**2)\n",
    "\n",
    "    print(f'    Generate noise with DCR per mm^2 {noisefreqpersqmm}, mean number of hits per event: {munoise:.2f}.')\n",
    "\n",
    "    noisehits = rng.poisson(munoise, nevents)   # генерация массива числа шумовых срабатываний в событиях по пуассоновскому распределению\n",
    "    Ndc = int(noisehits.sum())                  # общее число шумовых срабатываний (скаляр)\n",
    "    signalhits = partdf['nhits'].to_numpy()     # массив числа сигнальных срабатываний по событиям\n",
    "\n",
    "    # случайное смещение сигнальных срабатываний в пределах временного окна генерации шума\n",
    "    if shiftSignalTimes:\n",
    "      hitdf['t_c'] += np.repeat(rng.uniform(0, noiseTimeRange[1]-2, nevents), partdf['nhits'])\n",
    "\n",
    "    hitdf['signal'] = np.ones(signalhits.sum(), bool)  # разметка сигнальных срабатываний значением 'signal' True\n",
    "    if Ndc == 0:    # если нет шумовых срабатываний\n",
    "      return hitdf  # возвращаем исходный датафрейм с добавлением колонки 'signal'\n",
    "\n",
    "    ich = rng.choice(xgrid.size, Ndc)           # генерация случайных номеров сработавших каналов с возможным повтором\n",
    "    xh = xgrid[ich]                             # x-координата сработавших каналов\n",
    "    yh = ygrid[ich]                             # y-координата сработавших каналов\n",
    "    # zh = hitdf.at[(0, 0), 'z_c']                # z-координата срабатываний (скаляр)\n",
    "    zh = 201.050003\n",
    "    th = rng.uniform(noiseTimeRange[0], noiseTimeRange[1], size=Ndc) # генерация времён срабатываний по однородному распределению\n",
    "   \n",
    "    # нумерация шумовых срабатываний по событиям\n",
    "    ievent = np.repeat(partdf.index, noisehits) # массив номеров событий для записи в датафрейм\n",
    "    ihit = np.zeros(Ndc, 'int64')               # инициализация массива номеров срабатываний для записи в датафрейм\n",
    "    index = 0\n",
    "    for i in range(nevents):\n",
    "      ihit[index:index+noisehits[i]] = signalhits[i] + np.arange(noisehits[i])\n",
    "      index += noisehits[i]\n",
    "    \n",
    "    # создание датафрейма с шумовыми срабатываниями того же формата, что hitdf\n",
    "    noisedf = pd.DataFrame({'x_c': xh, 'y_c': yh, 'z_c': zh, 't_c': th, 'signal': np.zeros(Ndc, bool)},\n",
    "                           index=pd.MultiIndex.from_arrays((ievent, ihit), names=('entry', 'subentry')))\n",
    "\n",
    "    # TO DO: случайное смещение кольца в фотодетекторе (сдвиг координат сигнальных хитов).\n",
    "    # Сложность с реализацией для неравномерной сетки пикселей, т.к. зазоры между матрицами больше зазоров между пикселями в матрице.\n",
    "    # Проще сделать в моделировании.\n",
    "\n",
    "    # сливаем сигнальный и шумовой датафрейм и сортируем указатель событий и срабатываний\n",
    "    hitdf2 = pd.concat((hitdf, noisedf), copy=False).sort_index(level=('entry', 'subentry'))\n",
    "\n",
    "    # обновляем количества срабатываний в partdf, добавляя количества шумовых срабатываний по событиям\n",
    "    partdf['nhits'] += noisehits\n",
    "\n",
    "    return hitdf2\n",
    "\n",
    "\n",
    "  nFileEvents = idf.at[0, 'nevents']\n",
    "  # print(f'Processing ROOT file {filepath} with {nFileEvents} simulated events...', flush=True)\n",
    "  \n",
    "  # Цикл чтения кусков ROOT-файла\n",
    "  for partdf, hitdf, otherdf in zip(uproot.pandas.iterate(filepath, \"raw_data\", part_rename_map.keys(), entrysteps=eventchunksize),\n",
    "                           uproot.pandas.iterate(filepath, \"raw_data\", hit_rename_map.keys(), entrysteps=eventchunksize, flatten=True),\n",
    "                           uproot.pandas.iterate(filepath, \"raw_data\", other_rename_map.keys(), entrysteps=eventchunksize, flatten=True)):\n",
    "    # print('\\n  Processing next chunk...')\n",
    "\n",
    "    # Переименование колонок\n",
    "    partdf.rename(columns=part_rename_map, inplace=True, errors='raise')\n",
    "    hitdf.rename(columns=hit_rename_map, inplace=True, errors='raise')\n",
    "    otherdf.rename(columns=other_rename_map, inplace=True, errors='raise')\n",
    "\n",
    "    partdf = partdf.astype('float32', copy=False)\n",
    "    partdf['nhits'] = partdf['nhits'].astype('int32', copy=False)\n",
    "    hitdf = hitdf.astype('float32', copy=False)\n",
    "    otherdf = otherdf.astype('float32', copy=False)\n",
    "\n",
    "    # Генерация и добавление шумовых срабатываний\n",
    "    hitdf = addNoise(partdf, hitdf)\n",
    "\n",
    "    # print(f'    {hitdf.index.levels[0].size} entries with {hitdf.shape[0]} hits to process')\n",
    "\n",
    "    # Слияние данных событий и срабатываний\n",
    "    edf = hitdf.join(otherdf)\n",
    "    edf = edf.join(partdf, on='entry')\n",
    "    #edf = hitdf.join(partdf, on='entry')\n",
    "    #edf = edf.join(otherdf, on='entry')\n",
    "    if verbose:\n",
    "      pass\n",
    "      # print(edf)\n",
    "\n",
    "    if edfstore is not None:\n",
    "      # print(f'    Saving edf chunk...')\n",
    "      edfstore.put('edf', edf, format='table', append=True)\n",
    "\n",
    "    yield edf"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cCcplDgkbT1Y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898487147,
     "user_tz": -180,
     "elapsed": 645,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "outputId": "f6651bf4-214c-43f8-94e6-9a9bd43c5468",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Чтение и домоделирование одного \"куска\" данных\n",
    "#edf = next(genChunkFromRoot(filepath, 1450, noisefreqpersqmm=1000000))\n",
    "edf = next(genChunkFromRoot(filepath, 10000, noisefreqpersqmm=0))"
   ],
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Generate noise with DCR per mm^2 0, mean number of hits per event: 0.00.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def edf_to_bdf(edf_col: pd.Series, bdf: pd.DataFrame):\n",
    "  to_bdf = [sub.iloc[0] for _, sub in edf_col.groupby(level=0)]\n",
    "  bdf[edf_col.name] = pd.Series(to_bdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "                       x_c         y_c         z_c       t_c  signal   \nentry subentry                                                         \n0     0         -70.879997 -226.880005  201.050003  2.188593    True  \\\n      1         -53.279999 -309.920013  201.050003  2.396409    True   \n      2         -36.480000 -306.559998  201.050003  2.382249    True   \n      3         -49.919998 -184.960007  201.050003  2.073419    True   \n      4         -12.160000 -288.959991  201.050003  2.326930    True   \n...                    ...         ...         ...       ...     ...   \n9999  40        178.240005  -80.959999  201.050003  3.695349    True   \n      41        178.240005  -74.239998  201.050003  3.687293    True   \n      42        226.880005  -53.279999  201.050003  3.892318    True   \n      43        274.720001 -209.279999  201.050003  4.102964    True   \n      44        251.199997  -18.879999  201.050003  3.840951    True   \n\n                m_id_event  m_id_primary  m_hits.m_photon_id_pmt   \nentry subentry                                                     \n0     0                1.0        -211.0                   367.0  \\\n      1                1.0        -211.0                   394.0   \n      2                1.0        -211.0                   394.0   \n      3                1.0        -211.0                   399.0   \n      4                1.0        -211.0                   425.0   \n...                    ...           ...                     ...   \n9999  40           10000.0        -211.0                   643.0   \n      41           10000.0        -211.0                   643.0   \n      42           10000.0        -211.0                   704.0   \n      43           10000.0        -211.0                   728.0   \n      44           10000.0        -211.0                   735.0   \n\n                m_hits.m_photon_id_chip  m_hits.m_photon_id_layer  ...   \nentry subentry                                                     ...   \n0     0                            31.0                       4.0  ...  \\\n      1                             7.0                       4.0  ...   \n      2                            48.0                       4.0  ...   \n      3                            11.0                       2.0  ...   \n      4                            37.0                       3.0  ...   \n...                                 ...                       ...  ...   \n9999  40                           25.0                       1.0  ...   \n      41                           27.0                       3.0  ...   \n      42                            9.0                       4.0  ...   \n      43                           60.0                       4.0  ...   \n      44                            3.0                       4.0  ...   \n\n                     x_p       y_p  z_p      nx_p      ny_p      nz_p   \nentry subentry                                                          \n0     0         1.275724  1.576625  0.0 -0.073019 -0.695325  0.714977  \\\n      1         1.275724  1.576625  0.0 -0.073019 -0.695325  0.714977   \n      2         1.275724  1.576625  0.0 -0.073019 -0.695325  0.714977   \n      3         1.275724  1.576625  0.0 -0.073019 -0.695325  0.714977   \n      4         1.275724  1.576625  0.0 -0.073019 -0.695325  0.714977   \n...                  ...       ...  ...       ...       ...       ...   \n9999  40        1.499871 -1.121928  0.0  0.419547 -0.313757  0.851784   \n      41        1.499871 -1.121928  0.0  0.419547 -0.313757  0.851784   \n      42        1.499871 -1.121928  0.0  0.419547 -0.313757  0.851784   \n      43        1.499871 -1.121928  0.0  0.419547 -0.313757  0.851784   \n      44        1.499871 -1.121928  0.0  0.419547 -0.313757  0.851784   \n\n                    beta   theta_p     phi_p    momentum  \nentry subentry                                            \n0     0         0.966818  0.774205  4.607758  528.209290  \n      1         0.966818  0.774205  4.607758  528.209290  \n      2         0.966818  0.774205  4.607758  528.209290  \n      3         0.966818  0.774205  4.607758  528.209290  \n      4         0.966818  0.774205  4.607758  528.209290  \n...                  ...       ...       ...         ...  \n9999  40        0.987837  0.551415  5.641063  886.668091  \n      41        0.987837  0.551415  5.641063  886.668091  \n      42        0.987837  0.551415  5.641063  886.668091  \n      43        0.987837  0.551415  5.641063  886.668091  \n      44        0.987837  0.551415  5.641063  886.668091  \n\n[314038 rows x 39 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>x_c</th>\n      <th>y_c</th>\n      <th>z_c</th>\n      <th>t_c</th>\n      <th>signal</th>\n      <th>m_id_event</th>\n      <th>m_id_primary</th>\n      <th>m_hits.m_photon_id_pmt</th>\n      <th>m_hits.m_photon_id_chip</th>\n      <th>m_hits.m_photon_id_layer</th>\n      <th>...</th>\n      <th>x_p</th>\n      <th>y_p</th>\n      <th>z_p</th>\n      <th>nx_p</th>\n      <th>ny_p</th>\n      <th>nz_p</th>\n      <th>beta</th>\n      <th>theta_p</th>\n      <th>phi_p</th>\n      <th>momentum</th>\n    </tr>\n    <tr>\n      <th>entry</th>\n      <th>subentry</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>-70.879997</td>\n      <td>-226.880005</td>\n      <td>201.050003</td>\n      <td>2.188593</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>-211.0</td>\n      <td>367.0</td>\n      <td>31.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.275724</td>\n      <td>1.576625</td>\n      <td>0.0</td>\n      <td>-0.073019</td>\n      <td>-0.695325</td>\n      <td>0.714977</td>\n      <td>0.966818</td>\n      <td>0.774205</td>\n      <td>4.607758</td>\n      <td>528.209290</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-53.279999</td>\n      <td>-309.920013</td>\n      <td>201.050003</td>\n      <td>2.396409</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>-211.0</td>\n      <td>394.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.275724</td>\n      <td>1.576625</td>\n      <td>0.0</td>\n      <td>-0.073019</td>\n      <td>-0.695325</td>\n      <td>0.714977</td>\n      <td>0.966818</td>\n      <td>0.774205</td>\n      <td>4.607758</td>\n      <td>528.209290</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-36.480000</td>\n      <td>-306.559998</td>\n      <td>201.050003</td>\n      <td>2.382249</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>-211.0</td>\n      <td>394.0</td>\n      <td>48.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.275724</td>\n      <td>1.576625</td>\n      <td>0.0</td>\n      <td>-0.073019</td>\n      <td>-0.695325</td>\n      <td>0.714977</td>\n      <td>0.966818</td>\n      <td>0.774205</td>\n      <td>4.607758</td>\n      <td>528.209290</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-49.919998</td>\n      <td>-184.960007</td>\n      <td>201.050003</td>\n      <td>2.073419</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>-211.0</td>\n      <td>399.0</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>1.275724</td>\n      <td>1.576625</td>\n      <td>0.0</td>\n      <td>-0.073019</td>\n      <td>-0.695325</td>\n      <td>0.714977</td>\n      <td>0.966818</td>\n      <td>0.774205</td>\n      <td>4.607758</td>\n      <td>528.209290</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-12.160000</td>\n      <td>-288.959991</td>\n      <td>201.050003</td>\n      <td>2.326930</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>-211.0</td>\n      <td>425.0</td>\n      <td>37.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>1.275724</td>\n      <td>1.576625</td>\n      <td>0.0</td>\n      <td>-0.073019</td>\n      <td>-0.695325</td>\n      <td>0.714977</td>\n      <td>0.966818</td>\n      <td>0.774205</td>\n      <td>4.607758</td>\n      <td>528.209290</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">9999</th>\n      <th>40</th>\n      <td>178.240005</td>\n      <td>-80.959999</td>\n      <td>201.050003</td>\n      <td>3.695349</td>\n      <td>True</td>\n      <td>10000.0</td>\n      <td>-211.0</td>\n      <td>643.0</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.499871</td>\n      <td>-1.121928</td>\n      <td>0.0</td>\n      <td>0.419547</td>\n      <td>-0.313757</td>\n      <td>0.851784</td>\n      <td>0.987837</td>\n      <td>0.551415</td>\n      <td>5.641063</td>\n      <td>886.668091</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>178.240005</td>\n      <td>-74.239998</td>\n      <td>201.050003</td>\n      <td>3.687293</td>\n      <td>True</td>\n      <td>10000.0</td>\n      <td>-211.0</td>\n      <td>643.0</td>\n      <td>27.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>1.499871</td>\n      <td>-1.121928</td>\n      <td>0.0</td>\n      <td>0.419547</td>\n      <td>-0.313757</td>\n      <td>0.851784</td>\n      <td>0.987837</td>\n      <td>0.551415</td>\n      <td>5.641063</td>\n      <td>886.668091</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>226.880005</td>\n      <td>-53.279999</td>\n      <td>201.050003</td>\n      <td>3.892318</td>\n      <td>True</td>\n      <td>10000.0</td>\n      <td>-211.0</td>\n      <td>704.0</td>\n      <td>9.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.499871</td>\n      <td>-1.121928</td>\n      <td>0.0</td>\n      <td>0.419547</td>\n      <td>-0.313757</td>\n      <td>0.851784</td>\n      <td>0.987837</td>\n      <td>0.551415</td>\n      <td>5.641063</td>\n      <td>886.668091</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>274.720001</td>\n      <td>-209.279999</td>\n      <td>201.050003</td>\n      <td>4.102964</td>\n      <td>True</td>\n      <td>10000.0</td>\n      <td>-211.0</td>\n      <td>728.0</td>\n      <td>60.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.499871</td>\n      <td>-1.121928</td>\n      <td>0.0</td>\n      <td>0.419547</td>\n      <td>-0.313757</td>\n      <td>0.851784</td>\n      <td>0.987837</td>\n      <td>0.551415</td>\n      <td>5.641063</td>\n      <td>886.668091</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>251.199997</td>\n      <td>-18.879999</td>\n      <td>201.050003</td>\n      <td>3.840951</td>\n      <td>True</td>\n      <td>10000.0</td>\n      <td>-211.0</td>\n      <td>735.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.499871</td>\n      <td>-1.121928</td>\n      <td>0.0</td>\n      <td>0.419547</td>\n      <td>-0.313757</td>\n      <td>0.851784</td>\n      <td>0.987837</td>\n      <td>0.551415</td>\n      <td>5.641063</td>\n      <td>886.668091</td>\n    </tr>\n  </tbody>\n</table>\n<p>314038 rows × 39 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RnR-4_yXtH5v",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652898489590,
     "user_tz": -180,
     "elapsed": 277,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def recoAngles(edf: pd.DataFrame, idf: pd.DataFrame, rotation_mode = False):\n",
    "  '''\n",
    "  Геометрическая реконструкция углов фотонов относительно направления частицы.\n",
    "  Из координат срабатываний и частиц вычисляются углы theta_c, phi_c и время вылета фотонов t_c_orig и добавляются к edf.\n",
    "  '''\n",
    "  r0 = edf.loc[:, ('x_p', 'y_p', 'z_p')].to_numpy()\n",
    "  if rotation_mode:\n",
    "    r = edf.loc[:, ('rotated_x', 'rotated_y', 'rotated_z')].to_numpy()\n",
    "    # n0 = edf.loc[:, ('rotated_nx_p', 'rotated_ny_p', 'rotated_nz_p')].to_numpy()\n",
    "    n0 = edf.loc[:, ('recalculated_nx_p', 'recalculated_ny_p', 'recalculated_nz_p')].to_numpy()\n",
    "  else:\n",
    "    r  = edf.loc[:, ('x_c', 'y_c', 'z_c')].to_numpy()\n",
    "    n0 = edf.loc[:, ('nx_p', 'ny_p', 'nz_p')].to_numpy()\n",
    "\n",
    "  speedOfLight_mmperns = 299.792458 # мм/нс\n",
    "\n",
    "  # расстояние от радиатора до детектора\n",
    "  dist = float(idf['distance'])\n",
    "\n",
    "  # толщина радиатора\n",
    "  W = float(idf['W'])\n",
    "\n",
    "  # расстояние от точки вылета частицы до входной плоскости радиатора\n",
    "  rad_pos = float(idf['zdis'])\n",
    "\n",
    "  # полное число срабатываний\n",
    "  N = edf.shape[0]\n",
    "\n",
    "  # координаты точки пересечения трека с ФД\n",
    "  if not rotation_mode:\n",
    "    y_i = r0[:,1] + (dist + rad_pos) * n0[:,1] / n0[:,2] # r0[:,1] + (dist + W + rad_pos) * n0[:,1] / n0[:,2]   #   r0[:,1] + (dist + rad_pos) * n0[:,1] / n0[:,2]\n",
    "    x_i = r0[:,0] + (y_i - r0[:,1]) * n0[:,0] / n0[:,1] # r0[:,0] + (y_i - r0[:,1]) * n0[:,0] / n0[:,1]    #     r0[:,0] + (dist + rad_pos) * n0[:,0] / n0[:,2]\n",
    "    edf['x_i'] = x_i\n",
    "    edf['y_i'] = y_i\n",
    "    edf['r_p_c'] = np.sqrt((r0[:,0] - x_i) ** 2 + (r0[:,1] - y_i) ** 2 + (r0[:,2] - r[:,2]) ** 2)\n",
    "    edf['r_c'] = np.sqrt((x_i - edf['x_c']) ** 2 + (y_i - edf['y_c']) ** 2)\n",
    "\n",
    "  if rotation_mode:\n",
    "    n_mean = float(idf['n_mean'])\n",
    "\n",
    "    edf['rotated_r_c'] = np.sqrt((edf['rotated_x_i'] - edf['rotated_x']) ** 2 + (edf['rotated_y_i'] - edf['rotated_y']) ** 2)\n",
    "\n",
    "    rotated_r_c = edf['rotated_r_c'].to_numpy()\n",
    "    # r_p_c = edf['r_p_c'].to_numpy()\n",
    "    beta = edf['beta'].to_numpy()\n",
    "    r_p_c = dist # or + W/2 ???\n",
    "\n",
    "    edf['beta_from_true_r'] = np.sqrt(rotated_r_c ** 2 + r_p_c ** 2) / (n_mean * r_p_c)\n",
    "    edf['true_r_from_beta'] = r_p_c * np.sqrt((n_mean * beta) ** 2 - 1)\n",
    "\n",
    "    avg_betas = []\n",
    "    for _, subentry in edf['beta_from_true_r'].groupby(level=0):\n",
    "      avg_beta = subentry.mean()\n",
    "      for __ in subentry:\n",
    "        avg_betas.append(avg_beta)\n",
    "    edf['beta_from_true_r_mean'] = avg_betas\n",
    "  # косинусы и синусы сферических углов направления частицы\n",
    "  costheta, sintheta = n0[:,2], np.sqrt(n0[:,0]**2+n0[:,1]**2)\n",
    "  phi = np.arctan2(n0[:,1], n0[:,0])\n",
    "  cosphi, sinphi = np.cos(phi), np.sin(phi)\n",
    "\n",
    "  # номинальная точка вылета фотонов\n",
    "  ro = r0 + (W/2+rad_pos)/n0[:,2].reshape(N,1)*n0\n",
    "\n",
    "  \"\"\"\n",
    "  Преобразование в СК частицы\n",
    "  𝑢𝑥 = cos 𝜃(𝑣𝑥 cos 𝜙 + 𝑣𝑦 sin 𝜙) − 𝑣𝑧 sin 𝜃,\n",
    "  𝑢𝑦 = −𝑣𝑥 sin 𝜙 + 𝑣𝑦 cos 𝜙,\n",
    "  𝑢𝑧 = sin 𝜃(𝑣𝑥 cos 𝜙 + 𝑣𝑦 sin 𝜙) + 𝑣𝑧 cos 𝜃.\n",
    "  \"\"\"\n",
    "\n",
    "  # вектор направления фотона в лабораторной СК\n",
    "  s = (r-ro)\n",
    "  snorm = np.linalg.norm(s, axis=1, keepdims=True)\n",
    "  v = s / snorm\n",
    "  if not rotation_mode:\n",
    "    edf['t_c_orig'] = edf['t_c'] - (snorm / speedOfLight_mmperns).reshape(N)\n",
    "\n",
    "  # освобождение памяти при необходимости\n",
    "  #del r0, n0, ro, r, s\n",
    "\n",
    "  U = np.stack((np.stack((costheta*cosphi, costheta*sinphi, -sintheta)),\n",
    "                np.stack((-sinphi,         cosphi,          np.full(N, 0.))),\n",
    "                np.stack((sintheta*cosphi, sintheta*sinphi, costheta)))).transpose(2,0,1)\n",
    "\n",
    "  # единичный вектор направления фотона в СК частицы\n",
    "  u = (U @ v.reshape(N,3,1)).reshape(N,3)\n",
    "\n",
    "  # сферические углы фотона в СК частицы\n",
    "  if rotation_mode:\n",
    "    edf['rotated_theta_c'] = np.arccos(u[:,2])\n",
    "    edf['rotated_phi_c'] = np.arctan2(-u[:,1], -u[:,0])\n",
    "  else:\n",
    "    edf['theta_c'] = np.arccos(u[:,2])\n",
    "    edf['phi_c'] = np.arctan2(-u[:,1], -u[:,0])\n",
    "    avg_thetas = []\n",
    "    for _, subentry in edf['theta_c'].groupby(level=0):\n",
    "      avg_theta = subentry.mean()\n",
    "      for __ in subentry:\n",
    "        avg_thetas.append(avg_theta)\n",
    "    edf['theta_c_mean'] = avg_thetas"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def applySpaceCut(edf: pd.DataFrame) -> pd.DataFrame:\n",
    "  return edf[(abs(edf['x_c'] - edf['x_i']) <= 220) & (abs(edf['y_c'] - edf['y_i']) <= 220)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def signalLength(edf: pd.DataFrame):\n",
    "  t_signal = edf[edf['signal'] ==  True]['t_c']\n",
    "  t = []\n",
    "  for _, subentry in t_signal.groupby(level=0):\n",
    "    t_min = np.min(subentry)\n",
    "    t_max = np.max(subentry)\n",
    "    t.extend(t_max - t_min for __ in subentry)\n",
    "  t = np.array(t)\n",
    "  edf['signal_length'] = t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def planeRecalculation(edf: pd.DataFrame, idf: pd.DataFrame):\n",
    "    R_p = edf[['x_p', 'y_p', 'z_p']].to_numpy()\n",
    "    R = edf[['x_c', 'y_c', 'z_c']].to_numpy()\n",
    "    R_i = edf[['x_i', 'y_i', 'z_c']].to_numpy()\n",
    "    N = edf[['nx_p', 'ny_p', 'nz_p']].to_numpy()\n",
    "    dist = idf.W / 2 + idf.zdis\n",
    "    alpha = (float(dist) - R_p[:,2]) / N[:,2]\n",
    "    r_d = R_p + N * alpha[:, np.newaxis]\n",
    "\n",
    "    u = R - r_d\n",
    "    dot = np.sum(N * u, axis=1)\n",
    "    w = r_d - R_i\n",
    "    fac = -np.sum(N * w, axis=1) / dot\n",
    "    u *= fac[:, np.newaxis]\n",
    "\n",
    "    R_new = r_d + u\n",
    "\n",
    "\n",
    "\n",
    "    edf['recalculated_x'] = R_new[:,0]\n",
    "    edf['recalculated_y'] = R_new[:,1]\n",
    "    edf['recalculated_z'] = R_new[:,2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def primaryDirectionRecalculation(edf: pd.DataFrame):\n",
    "  N = edf.loc[:, ('nx_p', 'ny_p', 'nz_p')].to_numpy()\n",
    "  M = []\n",
    "  for n in N:\n",
    "    # C = np.stack((np.array([-(n[1] ** 2 + n[2] ** 2) / n[0], 0 , n[0]]),\n",
    "    #               np.array([n[1], - n[2], n[1]]),\n",
    "    #               np.array([n[2], n[1], n[2]])))\n",
    "\n",
    "    # C_inv = np.array([np.array([- n[0] / (n[0] ** 2 + n[1] ** 2 + n[2] ** 2), n[0] ** 2 * n[1] / (n[1] ** 4 + n[0] ** 2 * n[1] ** 2 + n[2] ** 4 + n[0] ** 2 * n[2] ** 2 + 2 * n[1] ** 2 * n[2] ** 2) , n[0] ** 2 * n[2] / (n[1] ** 4 + n[0] ** 2 * n[1] ** 2 + n[2] ** 4 + n[0] ** 2 * n[2] ** 2 + 2 * n[1] ** 2 * n[2] ** 2)]),\n",
    "    #               np.array([0, - n[2] / (n[1] ** 2 + n[2] ** 2), n[1] / (n[1] ** 2 + n[2] ** 2)]),\n",
    "    #               np.array([n[0] / (n[0] ** 2 + n[1] ** 2 + n[2] ** 2), n[1] / (n[0] ** 2 + n[1] ** 2 + n[2] ** 2), n[2] / (n[0] ** 2 + n[1] ** 2 + n[2] ** 2)])])\n",
    "    M.append([0, 0, 1])\n",
    "    # print(n)\n",
    "    # print(C_inv)\n",
    "    # print(C_inv @ n)\n",
    "    # break\n",
    "  M = np.array(M)\n",
    "  edf['recalculated_nx_p'] = M[:,0]\n",
    "  edf['recalculated_ny_p'] = M[:,1]\n",
    "  edf['recalculated_nz_p'] = M[:,2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def planeRotation(edf: pd.DataFrame):\n",
    "  R = edf[['recalculated_x', 'recalculated_y', 'recalculated_z']].to_numpy()\n",
    "  R_i = edf[['x_i', 'y_i', 'z_c']].to_numpy()\n",
    "  N = edf[['nx_p', 'ny_p', 'nz_p']].to_numpy() # N\n",
    "  M = np.array([0, 0, 1])                           # M\n",
    "  c = np.dot(N, M) / (np.linalg.norm(M) * np.linalg.norm(N, axis=1))\n",
    "  axis = np.cross(N, np.broadcast_to(M, (N.shape[0], 3))) / np.linalg.norm(np.cross(N, np.broadcast_to(M, (N.shape[0], 3))), axis=1, keepdims=True)\n",
    "  x, y, z = axis.T\n",
    "  s = np.sqrt(1-c*c)\n",
    "  C = 1-c\n",
    "  rmat = np.array([\n",
    "      [x*x*C+c, x*y*C-z*s, x*z*C+y*s],\n",
    "      [y*x*C+z*s, y*y*C+c, y*z*C-x*s],\n",
    "      [z*x*C-y*s, z*y*C+x*s, z*z*C+c]])\n",
    "  # print(rmat.shape)\n",
    "  # print(R.shape)\n",
    "  # print(rmat[:, :, 0])\n",
    "  # print(R[0])\n",
    "  # print(rmat[:, :, 0] @ R[0])\n",
    "  rotated_R = np.matmul(rmat.transpose((2, 0, 1)), R[:, :, np.newaxis])\n",
    "  rotated_R = np.squeeze(rotated_R, axis=-1).transpose().T\n",
    "  rotated_R_i = np.matmul(rmat.transpose((2, 0, 1)), R_i[:, :, np.newaxis])\n",
    "  rotated_R_i = np.squeeze(rotated_R_i, axis=-1).transpose().T\n",
    "  # print(rotated_R[0])\n",
    "  maskR = np.logical_or(abs(rotated_R[:, 0]) >= 500, abs(rotated_R[:, 1]) >= 500)\n",
    "  maskR_i = np.logical_or(abs(rotated_R_i[:, 0]) >= 500, abs(rotated_R_i[:, 1]) >= 500)\n",
    "  rotated_R[maskR] = [5000, 5000, 0]\n",
    "  rotated_R_i[maskR_i] = [5000, 5000, 0]\n",
    "  rotated_n = (rotated_R_i - edf[['x_p', 'y_p', 'z_p']].to_numpy()) / np.linalg.norm(rotated_R_i - edf[['x_p', 'y_p', 'z_p']].to_numpy(), axis=1, keepdims=True)\n",
    "  edf['rotated_x'] = rotated_R[:,0]\n",
    "  edf['rotated_y'] = rotated_R[:,1]\n",
    "  edf['rotated_z'] = rotated_R[:,2]\n",
    "  edf['rotated_x_i'] = rotated_R_i[:,0]\n",
    "  edf['rotated_y_i'] = rotated_R_i[:,1]\n",
    "  edf['rotated_z_i'] = rotated_R_i[:,2]\n",
    "  edf['rotated_nx_p'] = rotated_n[:,0]\n",
    "  edf['rotated_ny_p'] = rotated_n[:,1]\n",
    "  edf['rotated_nz_p'] = rotated_n[:,2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def applySecondSpaceCut(edf: pd.DataFrame) -> pd.DataFrame:\n",
    "  return edf[(abs(edf['rotated_x'] - edf['rotated_x_i']) <= 220) & (abs(edf['rotated_y'] - edf['rotated_y_i']) <= 220)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def local_sum_2d(event, r_slices, t_slices, square_counts, max_index, n, m, timestep, t_window_width, method='N/r'):\n",
    "  # cut_event = event[(event.t_c <= t_slices[np.clip(max_index[1] + m, a_min=0, a_max=10)]) & (event.t_c >= t_slices[np.clip(max_index[1] - m, a_min=0, a_max=10)]) &\n",
    "  #                   (event.rotated_r_c <= r_slices[max_index[0] + n]) & (event.rotated_r_c >= r_slices[max_index[0] - n])]\n",
    "  cut_event = event[(event.t_c <= np.clip(t_slices[max_index[1]] + t_window_width + timestep * m, 0, 10)) & (event.t_c >= np.clip(t_slices[max_index[1]] - timestep * m, 0, 10)) &\n",
    "                    (event.rotated_r_c <= r_slices[max_index[0] + n]) & (event.rotated_r_c >= r_slices[max_index[0] - n])]\n",
    "  return np.mean(cut_event.rotated_r_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def local_weighed_sum_2d(r_slices, t_slices, square_counts, max_index, n, m, method='N/r'):\n",
    "  arr = np.mean(square_counts[max_index[0] - n:max_index[0] + n + 1, np.clip(max_index[1] - m, 0, 50):np.clip(max_index[1] + m + 1, 0, 50)], axis=1)\n",
    "  if method == 'N/r':\n",
    "    sum_arr = r_slices[max_index[0] - n:max_index[0] + n + 1] ** 2 * arr\n",
    "    den_arr = r_slices[max_index[0] - n:max_index[0] + n + 1] * arr\n",
    "  else:\n",
    "    sum_arr = r_slices[max_index[0] - n:max_index[0] + n + 1] * arr\n",
    "    den_arr = arr\n",
    "\n",
    "  weighted_sum = np.sum(sum_arr)\n",
    "  weighted_den = np.sum(den_arr)\n",
    "\n",
    "  return weighted_sum / weighted_den"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def local_weighed_sum(r_slices, counts, max_index, n, method='N/r'):\n",
    "  if method == 'N/r':\n",
    "    sum_arr = r_slices[max_index - n:max_index + n + 1] ** 2 * counts[max_index - n:max_index + n + 1]\n",
    "    den_arr = r_slices[max_index - n:max_index + n + 1] * counts[max_index - n:max_index + n + 1]\n",
    "  else:\n",
    "    sum_arr = r_slices[max_index - n:max_index + n + 1] * counts[max_index - n:max_index + n + 1]\n",
    "    den_arr = counts[max_index - n:max_index + n + 1]\n",
    "\n",
    "  weighted_sum = np.sum(sum_arr)\n",
    "  weighted_den = np.sum(den_arr)\n",
    "\n",
    "  return weighted_sum / weighted_den"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def pol(x, a, b, c):\n",
    "  return a * np.exp((x - b) ** 2 / c ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def rSlidingWindowIntro(edf: pd.DataFrame, idf: pd.DataFrame, bdf: pd.DataFrame, avg_sigmas: tuple, avg_t_sigmas: tuple, step=float(idf.pixel_size), method='N/r', cal_arr=False, t_window_width=2,\n",
    "                        r_width_factor=2, t_width_factor=8, full_width_t_hist = False, num_of_groups=5):\n",
    "  r_r_c = edf['rotated_r_c']\n",
    "  time_step = float(t_window_width) / t_width_factor\n",
    "  all_avgs = np.array(r_r_c.groupby(level=0).transform('mean').to_list()).ravel()\n",
    "  all_dists = np.abs(r_r_c - all_avgs)\n",
    "  all_sigms = np.array(r_r_c.groupby(level=0).transform('std').to_list()).ravel()\n",
    "\n",
    "  edf['mean_rotated_r_c'] = all_avgs\n",
    "  edf['dist_from_mean_rotated_r_c'] = all_dists\n",
    "  edf['rotated_r_c_sigm'] = all_sigms\n",
    "\n",
    "  # Compute beta_step and r_step using NumPy functions\n",
    "  beta_step = np.ptp(edf['beta'].values) # не факт что нужно values\n",
    "  r_step = np.ptp(edf['true_r_from_beta'].values)\n",
    "\n",
    "  # Compute beta_intervals using NumPy linspace function\n",
    "  num_of_groups = num_of_groups\n",
    "  beta_intervals = np.linspace(edf['beta'].min(), edf['beta'].max(), num=num_of_groups)\n",
    "\n",
    "  # Compute beta_group_to_bdf and true_r_group using NumPy operations\n",
    "  beta_group = np.floor((num_of_groups * edf['beta'] + max(edf['beta']) - (num_of_groups + 1) * min(edf['beta'])) / beta_step).values\n",
    "  true_r_group = np.floor((num_of_groups * edf['true_r_from_beta'] + max(edf['true_r_from_beta']) - (num_of_groups + 1) * min(edf['true_r_from_beta'])) / r_step).values\n",
    "\n",
    "  edf['beta_group'] = beta_group\n",
    "  edf['true_r_group'] = true_r_group\n",
    "\n",
    "  edf_to_bdf(edf.beta_group, bdf)\n",
    "  edf_to_bdf(edf.true_r_group, bdf)\n",
    "\n",
    "  edf_to_bdf(edf.theta_p, bdf)\n",
    "  bdf['cos_theta_p'] = np.cos(bdf['theta_p'])\n",
    "  # edf_to_bdf(edf.signal_counts, bdf)\n",
    "  edf_to_bdf(edf.beta, bdf)\n",
    "  edf_to_bdf(edf.true_r_from_beta, bdf)\n",
    "\n",
    "  # edf['slice_group'] = np.floor(r_r_c / step) * step\n",
    "  edf['slice_group'] = r_r_c # test\n",
    "  # edf['time_slice_group'] = np.floor(edf.t_c / time_step) * time_step\n",
    "  edf['time_slice_group'] = edf.t_c # test2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def calculateSignalCounts(edf: pd.DataFrame, bdf: pd.DataFrame):\n",
    "  signal_counts = edf['signal'].groupby(level=0).sum()\n",
    "  bdf['signal_counts'] = signal_counts.values\n",
    "  edf['signal_counts'] = edf.signal.groupby(level=0).transform('sum').values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def rSlidingWindowLoop1(edf: pd.DataFrame, idf: pd.DataFrame, bdf: pd.DataFrame, avg_sigmas: tuple, avg_t_sigmas: tuple, step=float(idf.pixel_size), method='N/r', cal_arr=False, t_window_width=2,\n",
    "                        r_width_factor=2, t_width_factor=8, full_width_t_hist = True, weighed = True):\n",
    "  step = step / r_width_factor\n",
    "  time_step = float(t_window_width) / t_width_factor\n",
    "\n",
    "  r_slices = np.arange(0, 800, step=step)\n",
    "  t_slices = np.arange(0, 15, step=time_step)\n",
    "\n",
    "  n_sigmas = np.ptp(avg_sigmas)\n",
    "  t_sigmas = np.ptp(avg_t_sigmas)\n",
    "  # all_counts_to_edf = np.zeros((n_sigmas, len(edf)))\n",
    "  all_counts_to_edf = np.zeros((n_sigmas, len(edf)))\n",
    "  all_calculated_r = np.zeros((n_sigmas, len(edf)))\n",
    "  all_calculated_r_from_2d = np.zeros((t_sigmas, n_sigmas, len(edf)))\n",
    "  cur_ind = 0\n",
    "  for i, (entry, subentry) in enumerate(edf[['slice_group', 'time_slice_group', 'rotated_r_c', 't_c']].groupby(level=0)):\n",
    "    counts = np.zeros(r_slices.shape)\n",
    "    square_counts = np.zeros(shape=(r_slices.shape[0], t_slices.shape[0]))\n",
    "\n",
    "    mask = np.logical_and(subentry.slice_group >= 16, subentry.slice_group <= 80)\n",
    "    slice_groups = subentry.slice_group[mask]\n",
    "    time_slice_groups = subentry.time_slice_group[mask]\n",
    "\n",
    "    counts, _ = np.histogram(slice_groups, bins=r_slices)\n",
    "    square_counts, _, __ = np.histogram2d(slice_groups, time_slice_groups, bins=(r_slices, t_slices))\n",
    "\n",
    "    if method == 'N/r':\n",
    "      counts = np.divide(np.add(counts[:-1], counts[1:]), r_slices[1:-1])\n",
    "      square_counts[:-1, :] = np.divide(np.add(square_counts[:-1, :], square_counts[1:, :]), r_slices[1:-1, np.newaxis])\n",
    "    if full_width_t_hist:\n",
    "      square_counts_but_last = sum([square_counts[:, it : -t_width_factor + 1 + it] for it in range(t_width_factor - 1)])\n",
    "      square_counts = np.add(square_counts_but_last, square_counts[:, t_width_factor - 1:])\n",
    "\n",
    "    max_index = np.argmax(counts)\n",
    "\n",
    "    max_index_2d = np.unravel_index(np.argmax(square_counts), square_counts.shape)\n",
    "\n",
    "    for j in range(n_sigmas):\n",
    "      all_counts_to_edf[j][cur_ind:subentry.shape[0] + cur_ind] = counts[np.floor_divide(subentry.slice_group, step).astype(int)] # fixed\n",
    "      # avg_r_from_slices = local_weighed_sum(r_slices, counts, max_index, j + avg_sigmas[0], method)\n",
    "      # all_calculated_r[j, cur_ind:subentry.shape[0] + cur_ind] = np.repeat(avg_r_from_slices, subentry.shape[0])\n",
    "      for t in range(t_sigmas):\n",
    "        if weighed:\n",
    "          avg_r_from_2d_slices = local_weighed_sum_2d(r_slices, t_slices, square_counts, max_index_2d, j + avg_sigmas[0], t + avg_t_sigmas[0])\n",
    "        else:\n",
    "          avg_r_from_2d_slices = local_sum_2d(subentry, r_slices, t_slices, square_counts, max_index_2d, j + avg_sigmas[0], t + avg_t_sigmas[0], t_window_width=t_window_width, timestep=time_step)\n",
    "\n",
    "        # if np.isnan(avg_r_from_2d_slices):\n",
    "        #   print(max_index_2d)\n",
    "        #   print(square_counts[max_index_2d[0] - 1:max_index_2d[0] + 2, max_index_2d[1] - 1: max_index_2d[1] + 2])\n",
    "        #   raise ValueError\n",
    "        all_calculated_r_from_2d[t, j, cur_ind:subentry.shape[0] + cur_ind] = np.repeat(avg_r_from_2d_slices, subentry.shape[0])\n",
    "\n",
    "    cur_ind += subentry.shape[0]\n",
    "  for j in range(n_sigmas):\n",
    "    edf[f'slice_counts_{j + avg_sigmas[0]}_sigms'] = all_counts_to_edf[j]\n",
    "    # edf[f'unfixed_calculated_r_{j + avg_sigmas[0]}_sigms'] = all_calculated_r[j, :]\n",
    "    for t in range(t_sigmas):\n",
    "      edf[f'unfixed_calculated_r_2d_{j + avg_sigmas[0]}_rsigms_{t + avg_t_sigmas[0]}_tsigms'] = all_calculated_r_from_2d[t, j, :]\n",
    "      edf_to_bdf(edf[f'unfixed_calculated_r_2d_{j + avg_sigmas[0]}_rsigms_{t + avg_t_sigmas[0]}_tsigms'], bdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def rSlidingWindowLoop2(edf: pd.DataFrame, idf: pd.DataFrame, bdf: pd.DataFrame, avg_sigmas: tuple, avg_t_sigmas: tuple, step=float(idf.pixel_size), method='N/r', cal_arr=False, t_window_width=2,\n",
    "                        r_width_factor=2, t_width_factor=8, full_width_t_hist = False, param_fit=False):\n",
    "\n",
    "  # cal_arr = np.array([np.array([np.array(y) for y in x]) for x in cal_arr])\n",
    "\n",
    "  edf_to_bdf(edf.theta_p, bdf)\n",
    "  bdf['cos_theta_p'] = np.cos(bdf['theta_p'])\n",
    "  edf['cos_theta_p'] = np.cos(edf['theta_p'])\n",
    "  theta_interval = np.ptp(bdf.cos_theta_p) / 10\n",
    "  theta_min = min(bdf.cos_theta_p)\n",
    "  theta_max = max(bdf.cos_theta_p)\n",
    "  edf_to_bdf(edf.beta, bdf)\n",
    "  edf_to_bdf(edf.true_r_from_beta, bdf)\n",
    "\n",
    "  # for n_sigms in range(*avg_sigmas):\n",
    "  #   meas_betas = []\n",
    "  #   for entry, subentry in edf[f'unfixed_calculated_r_{n_sigms}_sigms'].groupby(level=0):\n",
    "  #     try:\n",
    "  #       meas_beta = pol(subentry.iloc[0], *(cal_arr[n_sigms - 1][(np.floor((np.cos(edf.theta_p[entry].iloc[0]) - theta_min) / (theta_interval))).astype(int)]))\n",
    "  #     except IndexError:\n",
    "  #       meas_beta = pol(subentry.iloc[0], *(cal_arr[n_sigms - 1][9]))\n",
    "  #       print((np.cos(edf.theta_p[entry].iloc[0]) - theta_min) / (theta_interval))\n",
    "  #     for subsub in subentry:\n",
    "  #       meas_betas.append(meas_beta)\n",
    "  #   edf[f'beta_from_calc_r_{n_sigms}_sigms'] = meas_betas\n",
    "  #\n",
    "  #   edf[f'delta_beta_{n_sigms}_sigms'] = edf[f'beta_from_calc_r_{n_sigms}_sigms'] - edf['beta']\n",
    "  #   edf[f'eps_beta_{n_sigms}_sigms'] = edf[f'delta_beta_{n_sigms}_sigms'] / edf['beta'] * 100\n",
    "  #\n",
    "  #   edf_to_bdf(edf[f'unfixed_calculated_r_{n_sigms}_sigms'], bdf)\n",
    "  #   edf_to_bdf(edf[f'beta_from_calc_r_{n_sigms}_sigms'], bdf)\n",
    "  #   edf_to_bdf(edf[f'delta_beta_{n_sigms}_sigms'], bdf)\n",
    "  #   edf_to_bdf(edf[f'eps_beta_{n_sigms}_sigms'], bdf)\n",
    "\n",
    "\n",
    "  for n_sigms in range(*avg_sigmas):\n",
    "    for t_sigms in range(*avg_t_sigmas):\n",
    "      meas_betas = np.zeros(edf.shape[0])\n",
    "      cur_ind = 0\n",
    "      # meas_betas = []\n",
    "      for entry, subentry in edf[f'unfixed_calculated_r_2d_{n_sigms}_rsigms_{t_sigms}_tsigms'].groupby(level=0):\n",
    "        if param_fit:\n",
    "          cos_theta_p = edf.cos_theta_p[entry].iloc[0]\n",
    "          meas_beta = pol(subentry.iloc[0], pol2(cos_theta_p, *cal_arr[n_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][0]),\n",
    "                          pol2(cos_theta_p, *cal_arr[n_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][1]),\n",
    "                          pol2(cos_theta_p, *cal_arr[n_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][2]))\n",
    "        else:\n",
    "          if edf.cos_theta_p[entry].iloc[0] != theta_max:\n",
    "            meas_beta = pol(subentry.iloc[0], *(cal_arr[n_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][(np.floor(((edf.cos_theta_p[entry].iloc[0]) - theta_min) / theta_interval)).astype(int)]))\n",
    "          else:\n",
    "            meas_beta = pol(subentry.iloc[0], *(cal_arr[n_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][9]))\n",
    "        meas_betas[cur_ind: subentry.shape[0] + cur_ind] = np.repeat(meas_beta, subentry.shape[0])\n",
    "        cur_ind += subentry.shape[0]\n",
    "        # for subsub in subentry:\n",
    "        #   meas_betas.append(meas_beta)  # FIXME\n",
    "      edf[f'beta_from_calc_r_{n_sigms}_rsigms_{t_sigms}_tsigms'] = meas_betas\n",
    "      edf[f'delta_beta_{n_sigms}_rsigms_{t_sigms}_tsigms'] = edf[f'beta_from_calc_r_{n_sigms}_rsigms_{t_sigms}_tsigms'] - edf['beta']\n",
    "      edf[f'eps_beta_{n_sigms}_rsigms_{t_sigms}_tsigms'] = edf[f'delta_beta_{n_sigms}_rsigms_{t_sigms}_tsigms'] / edf['beta'] * 100\n",
    "\n",
    "\n",
    "      edf_to_bdf(edf[f'beta_from_calc_r_{n_sigms}_rsigms_{t_sigms}_tsigms'], bdf)\n",
    "      edf_to_bdf(edf[f'delta_beta_{n_sigms}_rsigms_{t_sigms}_tsigms'], bdf)\n",
    "      edf_to_bdf(edf[f'eps_beta_{n_sigms}_rsigms_{t_sigms}_tsigms'], bdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def rSlidingWindow(edf: pd.DataFrame, idf: pd.DataFrame, bdf: pd.DataFrame, avg_sigmas: tuple, avg_t_sigmas: tuple, step=float(idf.pixel_size), method='N/r', cal_arr=False, t_window_width=2,\n",
    "                        r_width_factor=2, t_width_factor=8, full_width_t_hist = True, num_of_groups=5, weighed=True, deg_lim=False, param_fit=False):\n",
    "  rSlidingWindowIntro(edf, idf, bdf, avg_sigmas, avg_t_sigmas, step=step, method=method, cal_arr=cal_arr, t_window_width=t_window_width,\n",
    "                      r_width_factor=r_width_factor, t_width_factor=t_width_factor, num_of_groups=num_of_groups)\n",
    "  calculateSignalCounts(edf, bdf)\n",
    "  rSlidingWindowLoop1(edf, idf, bdf, avg_sigmas, avg_t_sigmas, step=step, method=method, cal_arr=cal_arr, t_window_width=t_window_width,\n",
    "                      r_width_factor=r_width_factor, t_width_factor=t_width_factor, full_width_t_hist=full_width_t_hist, weighed=weighed)\n",
    "  if cal_arr is False:\n",
    "    cal_arr = np.array(calibration(edf, bdf, avg_sigmas=avg_sigmas, avg_t_sigmas=avg_t_sigmas, step=step, t_window_width=t_window_width,\n",
    "                                   r_width_factor=r_width_factor, t_width_factor=t_width_factor, weighed=weighed, deg_lim=deg_lim, param_fit=param_fit)) # add r and t calibr - done\n",
    "  rSlidingWindowLoop2(edf, idf, bdf, avg_sigmas, avg_t_sigmas, step=step, method=method, cal_arr=cal_arr, t_window_width=t_window_width,\n",
    "                      r_width_factor=r_width_factor, t_width_factor=t_width_factor, param_fit=param_fit)\n",
    "  return cal_arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def rms90(arr):\n",
    "    # Calculate the mean and standard deviation of the array\n",
    "    arr = arr.dropna()\n",
    "    arr_mean = np.mean(arr)\n",
    "    arr_std = np.std(arr)\n",
    "\n",
    "    # Define the upper and lower limits for the 90% range\n",
    "    lower_limit = np.percentile(arr, 5)\n",
    "    upper_limit = np.percentile(arr, 95)\n",
    "    # print(lower_limit, upper_limit)\n",
    "    # print(arr)\n",
    "    # Select the values within the 90% range\n",
    "    arr_filtered = arr[(arr >= lower_limit) & (arr <= upper_limit)]\n",
    "    # print(arr_filtered)\n",
    "    assert arr_filtered.shape\n",
    "    # Calculate the root mean square of the filtered values\n",
    "    rms = np.std(arr_filtered)\n",
    "\n",
    "    return rms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def betaGroupsRMS90(bdf: pd.DataFrame, avg_sigmas: tuple, avg_t_sigmas: tuple, n = 5):\n",
    "  beta_sigms = np.full((np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), n), 0.)\n",
    "  beta_epss = np.full((np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), n), 0.)\n",
    "  beta_sigms_sigms = np.full((np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), n), 0.)\n",
    "\n",
    "  for group in range(1, n + 1):\n",
    "    data = bdf[bdf['beta_group'] == group]\n",
    "    for i in range(np.ptp(avg_sigmas)):\n",
    "      for j in range(np.ptp(avg_t_sigmas)):\n",
    "        population_fourth_moment = np.mean(bdf[f'delta_beta_{i + avg_sigmas[0]}_rsigms_{j + avg_t_sigmas[0]}_tsigms'] ** 4)\n",
    "        sample_fourth_moment = np.mean(data[f'delta_beta_{i + avg_sigmas[0]}_rsigms_{j + avg_t_sigmas[0]}_tsigms'] ** 4)\n",
    "        # print(np.std(data[f'delta_beta_{i + avg_sigmas[0]}_rsigms_{j + avg_t_sigmas[0]}_tsigms']))\n",
    "        beta_sigms[i, j, group - 1] = rms90(data[f'delta_beta_{i + avg_sigmas[0]}_rsigms_{j + avg_t_sigmas[0]}_tsigms'])\n",
    "        # assert not np.isnan(beta_sigms[i, j, group - 1])\n",
    "        beta_epss[i, j, group - 1] = rms90(data[f'eps_beta_{i + avg_sigmas[0]}_rsigms_{j + avg_t_sigmas[0]}_tsigms'])\n",
    "        beta_sigms_sigms[i, j, group - 1] = np.sqrt(2 * np.abs(sample_fourth_moment - population_fourth_moment) / (data.shape[0]))\n",
    "\n",
    "  return beta_sigms, beta_epss, beta_sigms_sigms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# labels = ['0', '1e3', '1e4', '1e5', '1e6']\n",
    "# labels = ['DCR = ' + i + ' $Hz/mm^2$' for i in labels]\n",
    "# num_of_groups = 10\n",
    "# y = np.arange(1, num_of_groups + 1)\n",
    "# x = (y * (max(edf['beta']) - min(edf['beta'])) - max(edf['beta']) + (num_of_groups + 1) * min(edf['beta'])) / num_of_groups\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(len(labels)):\n",
    "#   plt.plot(x, [0.0005] * 10, label=labels[i])\n",
    "# plt.ylim((0, 0.002))\n",
    "# # plt.xlim((0.954, 0.999))\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def plot_final_graph(beta_sigms, beta_sigms_yerr, avg_sigmas, avg_t_sigmas, r_width, t_width, r_factor, t_factor, weighed, to_save=True, deg_lim=False, num_of_groups=10):\n",
    "  labels = ['0', '1e3', '1e4', '1e5', '1e6']\n",
    "  labels = ['DCR = ' + i + ' $Hz/mm^2$' for i in labels]\n",
    "  colors = ['c', 'y', 'g', 'r', 'm']\n",
    "  weight = 'weighed' if weighed else 'unweighed'\n",
    "  y = np.arange(1, num_of_groups + 1)\n",
    "  x = (y * (max(edf['beta']) - min(edf['beta'])) - max(edf['beta']) + (num_of_groups + 1) * min(edf['beta'])) / num_of_groups\n",
    "\n",
    "  fig, axs = plt.subplots(np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), figsize=(10 * np.ptp(avg_t_sigmas), 10 * np.ptp(avg_sigmas)))\n",
    "  title = f'Method: N(r) / r; {weight} Avg\\nR Width = {r_width}mm, T Width = {t_width}ns\\nR step factor = {r_factor}, T step factor = {t_factor}'\n",
    "  if deg_lim:\n",
    "    title += '\\n' + r'$\\theta_p < 10\\deg$'\n",
    "  # fig.suptitle(title)\n",
    "\n",
    "  if np.ptp(avg_sigmas) > 1:\n",
    "    for i in range(np.ptp(avg_sigmas)):\n",
    "      for j in range(np.ptp(avg_t_sigmas)):\n",
    "        for k in range(beta_sigms.shape[0]):\n",
    "          axs[i, j].plot(x, beta_sigms[k, i, j], label=labels[k], c=colors[k])\n",
    "          axs[i, j].errorbar(x, beta_sigms[k, i, j], xerr=[np.diff(x)[0]/4 for _ in x], linestyle='', c=colors[k])\n",
    "          axs[i, j].errorbar(x, beta_sigms[k, i, j], yerr=beta_sigms_yerr[k, i, j], linestyle='', c=colors[k])\n",
    "        axs[i, j].legend(loc='upper right')\n",
    "        axs[i, j].set_xlabel('Beta Group')\n",
    "        axs[i, j].set_ylabel(r'RMS90($\\Delta\\beta$)')\n",
    "        if deg_lim:\n",
    "          axs[i, j].set_ylim((0, 0.004))\n",
    "        axs[i, j].set_title(f'Velocity resoultion for\\nr window width = {avg_sigmas[0] + i}$\\sigma$\\nt window width = {avg_t_sigmas[0] + j}$\\sigma$')\n",
    "        axs[i, j].grid()\n",
    "  elif np.ptp(avg_t_sigmas) > 1:\n",
    "    for j in range(np.ptp(avg_t_sigmas)):\n",
    "      for k in range(beta_sigms.shape[0]):\n",
    "        axs[j].plot(x, beta_sigms[k, 0, j], label=labels[k], c=colors[k])\n",
    "        axs[j].errorbar(x, beta_sigms[k, 0, j], xerr=[np.diff(x)[0]/4 for _ in x], linestyle='', c=colors[k])\n",
    "        axs[j].errorbar(x, beta_sigms[k, 0, j], yerr=beta_sigms_yerr[k, 0, j], linestyle='', c=colors[k])\n",
    "      axs[j].legend(loc='upper right')\n",
    "      axs[j].set_xlabel('Beta Group')\n",
    "      axs[j].set_ylabel(r'RMS90($\\Delta\\beta)$')\n",
    "      if deg_lim:\n",
    "        axs[j].set_ylim((0, 0.004))\n",
    "      axs[j].set_title(f'Velocity resoultion for\\nr window width = {avg_sigmas[0]}$\\sigma$\\nt window width = {avg_t_sigmas[0] + j}$\\sigma$')\n",
    "      axs[j].grid()\n",
    "  else:\n",
    "    for k in range(beta_sigms.shape[0]):\n",
    "      axs.plot(x, beta_sigms[k, 0, 0], label=labels[k], c=colors[k])\n",
    "      axs.errorbar(x, beta_sigms[k, 0, 0], xerr=[np.diff(x)[0]/4 for _ in x], linestyle='', c=colors[k])\n",
    "      axs.errorbar(x, beta_sigms[k, 0, 0], yerr=beta_sigms_yerr[k, 0, 0], linestyle='', c=colors[k])\n",
    "    axs.legend(loc='upper right')\n",
    "    axs.set_xlabel('Beta Group')\n",
    "    axs.set_ylabel(r'RMS90($\\Delta\\beta$)')\n",
    "    if deg_lim:\n",
    "      axs.set_ylim((0, 0.002))\n",
    "    axs.set_title(f'Velocity resoultion for\\nr window width = {avg_sigmas[0]}$\\sigma$\\nt window width = {avg_t_sigmas[0]}$\\sigma$')\n",
    "    axs.grid()\n",
    "\n",
    "  if to_save:\n",
    "    filename = f'{weight}_avg_rw={r_width}_tw={t_width}_rs={r_factor}_ts={t_factor}_rsigms={avg_sigmas[0]}-{avg_sigmas[-1]-1}_tsigms={avg_t_sigmas[0]}-{avg_t_sigmas[-1]-1}'\n",
    "    if deg_lim:\n",
    "      filename += '_10deg'\n",
    "    filename += '.png'\n",
    "    fig.savefig(os.path.join('results', f'{filename}'))\n",
    "    plt.close(fig)\n",
    "  else:\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def plot_groupped_distributions(bdf, avg_sigmas, avg_t_sigmas, r_width, t_width, r_factor, t_factor, weighed, background, to_save=True, deg_lim=False, num_of_groups=10):\n",
    "  labels = ['0', '1e3', '1e4', '1e5', '1e6']\n",
    "  labels = ['DCR = ' + i + ' cps' for i in labels]\n",
    "  colors = ['c', 'y', 'g', 'r', 'm']\n",
    "  weight = 'weighed' if weighed else 'unweighed'\n",
    "  left_lim = min(bdf[f'delta_beta_{avg_sigmas[0]}_rsigms_{avg_t_sigmas[0]}_tsigms'])\n",
    "  right_lim = max(bdf[f'delta_beta_{avg_sigmas[0]}_rsigms_{avg_t_sigmas[0]}_tsigms'])\n",
    "  y = np.arange(1, num_of_groups + 1)\n",
    "  x = (y * (max(edf['beta']) - min(edf['beta'])) - max(edf['beta']) + (num_of_groups + 1) * min(edf['beta'])) / num_of_groups\n",
    "  x_interval_over_2 = (x[1] - x[0]) / 2\n",
    "  fig, axs = plt.subplots(num_of_groups, 1, figsize=(16, 9 * num_of_groups), sharex=True)\n",
    "  title = f'Method: N(r) / r; {weight} Avg\\nR Width = {r_width}mm, T Width = {t_width}ns\\nR step factor = {r_factor}, T step factor = {t_factor}, DCR={background}'\n",
    "  if deg_lim:\n",
    "    title += '\\n' + r'$\\theta_p < 10\\deg$'\n",
    "  # fig.suptitle(title)\n",
    "  fig.tight_layout()\n",
    "  if np.ptp(avg_sigmas) > 1:\n",
    "    pass\n",
    "  elif np.ptp(avg_t_sigmas) > 1:\n",
    "    pass\n",
    "  else:\n",
    "    for group in range(1, num_of_groups + 1):\n",
    "      data = bdf[bdf['beta_group'] == group]\n",
    "      # data = data.dropna()\n",
    "      # print(data.shape)\n",
    "      # bin_heights, bin_borders = np.histogram(data[f'delta_beta_{avg_sigmas[0]}_rsigms_{avg_t_sigmas[0]}_tsigms'], bins='auto', normed=True)\n",
    "      # bin_width = np.diff(bin_borders)\n",
    "      axs[group - 1].hist(data[f'delta_beta_{avg_sigmas[0]}_rsigms_{avg_t_sigmas[0]}_tsigms'], bins='auto')\n",
    "      # axs[group - 1].set_xlabel(r'$\\Delta\\beta$')\n",
    "      axs[group - 1].set_ylabel(r'Events')\n",
    "      axs[group - 1].set_title(r'$\\beta$ in ' + f'[{round(x[group - 1] - x_interval_over_2, 3)}, {round(x[group -1 ] + x_interval_over_2, 3)}]')\n",
    "      axs[group - 1].set_xlim((-0.05, 0.08))\n",
    "\n",
    "  if to_save:\n",
    "    filename = f'Hist_{weight}_avg_rw={r_width}_tw={t_width}_rs={r_factor}_ts={t_factor}_rsigms={avg_sigmas[0]}-{avg_sigmas[-1]-1}_tsigms={avg_t_sigmas[0]}-{avg_t_sigmas[-1]-1}_background={background}'\n",
    "    if deg_lim:\n",
    "      filename += '_10deg'\n",
    "    filename += '.png'\n",
    "    fig.savefig(os.path.join('results', f'{filename}'))\n",
    "    plt.close(fig)\n",
    "  else:\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# print([[[]]* 5] * 5)\n",
    "# print(cal_arr.shape) # r_sigms -> t_sigms -> thetq_groups -> 3 params\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def pol2old(x, b, c, d):\n",
    "  return b * x ** 2 + c * x + d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def pol2(x, p0, p1, p2):\n",
    "  return p0 + p1 * x + p2 * x ** 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def d3pol2(X, p0, p1, p2, q0, q1, q2, k0, k1, k2):\n",
    "  r, theta = X\n",
    "  # return pol(r, pol2(theta, p0, p1, p2), pol2(theta, q0, q1, q2), pol2(theta, k0, k1, k2))\n",
    "  return pol(r, p0 + p1 * theta + p2 * theta ** 2, q0 + q1 * theta + q2 * theta ** 2, k0 + k1 * theta + k2 * theta ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def calibration(edf: pd.DataFrame, bdf: pd.DataFrame, avg_sigmas: tuple, avg_t_sigmas: tuple, step=float(idf.pixel_size), method='N/r', t_window_width=2,\n",
    "                        r_width_factor=2, t_width_factor=8, full_width_t_hist = False, weighed=True, deg_lim=False, param_fit=False):\n",
    "  def gaussian(x, mean, sigma):\n",
    "    return norm.pdf(x, loc=mean, scale=sigma)\n",
    "  # to_return_unbinned = [[], [], [], []]\n",
    "  bdf['cos_theta_p'] = np.cos(bdf['theta_p'])\n",
    "  theta_p_max = max(bdf.cos_theta_p)\n",
    "  theta_p_min = min(bdf.cos_theta_p)\n",
    "  num_of_theta_intervals = 11\n",
    "  theta_intervals = np.linspace(theta_p_min, theta_p_max, num=num_of_theta_intervals)\n",
    "  theta_dif = (theta_intervals[1:] + theta_intervals[:-1]) / 2\n",
    "  to_return_unbinned = np.full((np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), num_of_theta_intervals - 1, 3), 0.)\n",
    "\n",
    "  errs_tmp = np.full((np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), num_of_theta_intervals - 1, 3), 0.)\n",
    "  fit_params = np.full((np.ptp(avg_sigmas), np.ptp(avg_t_sigmas), 3, 3), 0.)\n",
    "\n",
    "  weight = 'weighed' if weighed else 'unweighed'\n",
    "  beta_min = min(bdf.beta)\n",
    "  num_of_beta_intervals = 10 # was 20\n",
    "  beta_delim = (max(bdf.beta) - min(bdf.beta)) / num_of_beta_intervals\n",
    "  beta_intervals = np.linspace(min(bdf.beta), max(bdf.beta), num=num_of_beta_intervals + 1)\n",
    "  dir_to_save = f'{weight}_rw={step}_tw={t_window_width}_rs={r_width_factor}_ts={t_width_factor}'\n",
    "  if not os.path.exists(os.path.join('calibrations', dir_to_save)):\n",
    "    os.mkdir(os.path.join('calibrations', dir_to_save))\n",
    "  for r_sigms in range(*avg_sigmas):\n",
    "    fig, axs = plt.subplots(num_of_theta_intervals - 1, np.ptp(avg_t_sigmas), figsize=(16 * np.ptp(avg_t_sigmas), 90))\n",
    "    for t_sigms in range(*avg_t_sigmas):\n",
    "      chosen_column = f'unfixed_calculated_r_2d_{r_sigms}_rsigms_{t_sigms}_tsigms'\n",
    "\n",
    "      meas_r_min = min(bdf[chosen_column])\n",
    "      meas_r_max = max(bdf[chosen_column])\n",
    "      num_of_meas_r_intervals = num_of_beta_intervals\n",
    "      meas_r_delim = (meas_r_max - meas_r_min) / num_of_meas_r_intervals\n",
    "      meas_r_intervals = np.linspace(meas_r_min, meas_r_max, num=num_of_meas_r_intervals + 1)\n",
    "\n",
    "\n",
    "      # fig2, axs2 = plt.subplots(10* num_of_theta_intervals - 10, 10, figsize=(16, 90))\n",
    "\n",
    "      for theta_interval_index in range(num_of_theta_intervals - 1):\n",
    "        xerrs = []\n",
    "        yerrs = []\n",
    "        gauss_beta = []\n",
    "        gauss_r = []\n",
    "        t_bdf = bdf.copy()\n",
    "        t_bdf = t_bdf[np.isfinite(t_bdf[chosen_column])]\n",
    "        t_bdf = t_bdf[t_bdf.signal_counts >= 15]\n",
    "        t_bdf = t_bdf[t_bdf.cos_theta_p <= theta_intervals[theta_interval_index + 1]]\n",
    "        t_bdf = t_bdf[t_bdf.cos_theta_p >= theta_intervals[theta_interval_index]]\n",
    "        if t_sigms - avg_t_sigmas[0] != 0:\n",
    "          for_colorbar = axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].hist2d(t_bdf[chosen_column], t_bdf.beta, bins=70, range=((0, 80), (beta_min,1)))\n",
    "          fig.colorbar(for_colorbar[3], ax=axs[theta_interval_index, t_sigms - avg_t_sigmas[0]])\n",
    "        else:\n",
    "          for_colorbar = axs[theta_interval_index].hist2d(t_bdf[chosen_column], t_bdf.beta, bins=70, range=((0, 80), (beta_min,1)))\n",
    "          fig.colorbar(for_colorbar[3], ax=axs[theta_interval_index])\n",
    "        t_bdf = t_bdf[t_bdf[chosen_column] <= 65]\n",
    "        t_bdf = t_bdf[t_bdf[chosen_column] >= 25]\n",
    "\n",
    "        pol_param, cov = curve_fit(pol, t_bdf[chosen_column], t_bdf.beta, maxfev=50000, p0=(1, 80, 30))\n",
    "        to_return_unbinned[r_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][theta_interval_index] = pol_param\n",
    "        pol_param_errs = np.sqrt(np.diag(cov))\n",
    "        errs_tmp[r_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][theta_interval_index] = pol_param_errs\n",
    "        rs = np.linspace(10, 80, num=50)\n",
    "        chi2 = np.sum((t_bdf.beta - pol(t_bdf[chosen_column], *pol_param)) ** 2)\n",
    "\n",
    "        if t_sigms - avg_t_sigmas[0] != 0:\n",
    "          axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].plot(rs, pol(rs, *pol_param), label=r'$\\chi^2$ = '+ str(chi2), c='r')\n",
    "          axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].set_xlim((0, 90))\n",
    "          axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].set_ylim((0.955, 1))\n",
    "          axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].set_ylim((beta_min, 1))\n",
    "          axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].set_xlabel(r'$R_{reco}$, mm')\n",
    "          axs[theta_interval_index, t_sigms - avg_t_sigmas[0]].set_ylabel(r'$\\beta_{true}$')\n",
    "        else:\n",
    "          axs[theta_interval_index].plot(rs, pol(rs, *pol_param), label=r'$\\chi^2$ = '+ str(chi2), c='r')\n",
    "          axs[theta_interval_index].set_xlim((0, 90))\n",
    "          axs[theta_interval_index].set_ylim((0.955, 1))\n",
    "          axs[theta_interval_index].set_ylim((beta_min, 1))\n",
    "          axs[theta_interval_index].set_xlabel(r'$R_{reco}$, mm')\n",
    "          axs[theta_interval_index].set_ylabel(r'$\\beta_{true}$')\n",
    "      if param_fit:\n",
    "        t_bdf = bdf.copy()\n",
    "        t_bdf = t_bdf[np.isfinite(t_bdf[chosen_column])]\n",
    "        t_bdf = t_bdf[t_bdf.signal_counts >= 5]\n",
    "        X = (np.array(t_bdf[chosen_column]), np.array(t_bdf.cos_theta_p))\n",
    "        fit, errs = curve_fit(d3pol2, X, t_bdf.beta, p0=(1.219, -0.5588, 0.2946, 864.4, -1922, 1055, -2535, 6572, -3751))\n",
    "        errs = np.sqrt(np.diag(errs))\n",
    "        for param in range(3):\n",
    "          fit_params[r_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][param] = fit[param * 3: param * 3 + 3]\n",
    "        print(fit)\n",
    "        print(errs)\n",
    "        chi2 = np.sum((t_bdf.beta - d3pol2(X, *fit)) ** 2)\n",
    "        print(chi2)\n",
    "      # if param_fit:\n",
    "      #   for param in range(3):\n",
    "      #     fit, _ = curve_fit(pol2, theta_dif, to_return_unbinned[r_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][:, param], sigma=errs_tmp[r_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][:, param])\n",
    "      #     fit_params[r_sigms - avg_sigmas[0]][t_sigms - avg_t_sigmas[0]][param] = fit\n",
    "    filename = f'rsigm={r_sigms}_t_sigms={avg_t_sigmas[0]}-{avg_t_sigmas[-1] - 1}'\n",
    "    if deg_lim:\n",
    "      filename += '_10deg'\n",
    "    if dir_to_save != '':\n",
    "      fig.savefig(os.path.join('calibrations', dir_to_save, f'{filename}'))\n",
    "      plt.close(fig)\n",
    "\n",
    "\n",
    "  if param_fit:\n",
    "    return fit_params\n",
    "  return to_return_unbinned\n",
    "# cal_arr = np.array(calibration(edf, bdf, avg_sigmas=(1, 5), avg_t_sigmas=(1, 5))) # add r and t calibr - done"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ы' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [88], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mы\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'ы' is not defined"
     ]
    }
   ],
   "source": [
    "ы"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "  print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# With DCR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f4VM3-a7N3wA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652899028826,
     "user_tz": -180,
     "elapsed": 22573,
     "user": {
      "displayName": "Платон Дмитриевич Рогожин",
      "userId": "06374754299160062301"
     }
    },
    "outputId": "9c993c1f-f0b0-40cf-c4f8-0190fe507ae2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "event = 1444\n",
    "#momentum = edf.at[(event, 0), 'momentum']\n",
    "#theta_p = edf.at[(event, 0), 'theta_p']*180/np.pi\n",
    "avg_sigmas=(4, 5)\n",
    "avg_t_sigmas=(4, 5)\n",
    "num_of_groups = 10\n",
    "r_width = float(idf.pixel_size)\n",
    "t_width = 0.25\n",
    "t_step = 0.25\n",
    "r_factor = 2 # not to change\n",
    "t_factor = int(t_width / t_step)\n",
    "weighed = False\n",
    "deg_lim = False\n",
    "param_fit = False\n",
    "for t_width in [0.25]: # [0.25, 0.5, 0.75, 1]\n",
    "  for t_step in [0.25]: # [0.25, 0.5]\n",
    "    for r_width in [2 * float(idf.pixel_size)]: # [float(idf.pixel_size), 2 * float(idf.pixel_size)]\n",
    "      for weighed in [True]: # [False, True]\n",
    "        print(f'Generating: {t_width}_{t_step}_{r_width}_{weighed}\\n')\n",
    "        cal_arr_for_dcr = False\n",
    "        beta_sigms = []\n",
    "        beta_sigms_yerr = []\n",
    "        beta_sigms_deglim = []\n",
    "        beta_sigms_yerr_deglim = []\n",
    "        for dcr in ['0']  : # ['0', '1e3', '1e4', '1e5', '1e6']\n",
    "          timer_start = perf_counter()\n",
    "          gen = genChunkFromRoot(filepath, 10000, noisefreqpersqmm=float(dcr), shiftSignalTimes=True)\n",
    "          c_bdf_d = pd.DataFrame()\n",
    "          for i in range(1):\n",
    "            edf_d = next(gen)\n",
    "            # edf_d = next(genChunkFromRoot(filepath, 10000, noisefreqpersqmm=float(dcr), shiftSignalTimes=True))\n",
    "\n",
    "            bdf_d = pd.DataFrame()\n",
    "            # signalLength(edf_d)\n",
    "            recoAngles(edf_d, idf)\n",
    "            edf_d = applySpaceCut(edf_d)\n",
    "            planeRecalculation(edf_d, idf)\n",
    "            planeRotation(edf_d)\n",
    "            edf_d = applySecondSpaceCut(edf_d)\n",
    "            primaryDirectionRecalculation(edf_d)\n",
    "\n",
    "            recoAngles(edf_d, idf, rotation_mode=True)\n",
    "            cal_arr_for_dcr = rSlidingWindow(edf_d, idf, bdf_d, avg_sigmas=avg_sigmas, avg_t_sigmas=avg_t_sigmas, cal_arr=cal_arr_for_dcr, num_of_groups=num_of_groups,\n",
    "                                             step=r_width, t_window_width=t_width, r_width_factor=r_factor, t_width_factor=t_factor, weighed=weighed, deg_lim=deg_lim, param_fit=param_fit)\n",
    "\n",
    "            if deg_lim:\n",
    "              edf_d = edf_d[edf_d.theta_p <= 10. * np.pi / 180]\n",
    "              edf_d = edf_d[edf_d.signal_counts >= 5]\n",
    "              bdf_d = bdf_d[bdf_d.theta_p <= 10. * np.pi / 180]\n",
    "              bdf_d = bdf_d[bdf_d.signal_counts >= 5]\n",
    "            if i == 0:\n",
    "              c_bdf_d = bdf_d\n",
    "            else:\n",
    "              c_bdf_d = pd.concat([c_bdf_d, bdf_d], ignore_index=True)\n",
    "          plot_groupped_distributions(c_bdf_d, avg_sigmas, avg_t_sigmas, r_width, t_width, r_factor, t_factor, weighed, background=dcr, deg_lim=False, num_of_groups=num_of_groups, to_save=True)\n",
    "          bg = betaGroupsRMS90(c_bdf_d, avg_sigmas=avg_sigmas, avg_t_sigmas=avg_t_sigmas, n=num_of_groups)\n",
    "          beta_sigms.append(bg[0])\n",
    "          beta_sigms_yerr.append(bg[2])\n",
    "\n",
    "          c_bdf_d = c_bdf_d[c_bdf_d.theta_p <= 10. * np.pi / 180]\n",
    "          c_bdf_d = c_bdf_d[c_bdf_d.signal_counts >= 5]\n",
    "\n",
    "          plot_groupped_distributions(c_bdf_d, avg_sigmas, avg_t_sigmas, r_width, t_width, r_factor, t_factor, weighed, background=dcr, deg_lim=True, num_of_groups=num_of_groups, to_save=True)\n",
    "          bg_deglim = betaGroupsRMS90(c_bdf_d, avg_sigmas=avg_sigmas, avg_t_sigmas=avg_t_sigmas, n=num_of_groups)\n",
    "          beta_sigms_deglim.append(bg_deglim[0])\n",
    "          beta_sigms_yerr_deglim.append(bg_deglim[2])\n",
    "          print(perf_counter() - timer_start)\n",
    "\n",
    "          # plothits(edf_d, 1, event, dir_to_save=f'event_{event}_{dcr}_noise')\n",
    "\n",
    "          #figxy.savefig(os.path.join(picsdir, f'labeled_ring_pi_p{momentum:.0f}mev_theta{theta_p:.0f}deg_dcr{dcr:.3g}.png'))\n",
    "          #figtime.savefig(os.path.join(picsdir, f'labeled_time_pi_p{momentum:.0f}mev_theta{theta_p:.0f}deg_dcr{dcr:.3g}.png'))\n",
    "        beta_sigms = np.array(beta_sigms)\n",
    "        beta_sigms_yerr = np.array(beta_sigms_yerr)\n",
    "        beta_sigms_deglim = np.array(beta_sigms_deglim)\n",
    "        beta_sigms_yerr_deglim = np.array(beta_sigms_yerr_deglim)\n",
    "        plot_final_graph(beta_sigms, beta_sigms_yerr, avg_sigmas, avg_t_sigmas, r_width, t_width, r_factor, t_factor, weighed, deg_lim=False, num_of_groups=num_of_groups)\n",
    "\n",
    "        plot_final_graph(beta_sigms_deglim, beta_sigms_yerr_deglim, avg_sigmas, avg_t_sigmas, r_width, t_width, r_factor, t_factor, weighed, deg_lim=True, num_of_groups=num_of_groups)"
   ],
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: 0.25_0.25_6.32_True\n",
      "\n",
      "    Generate noise with DCR per mm^2 0.0, mean number of hits per event: 0.00.\n",
      "29.183363300000565\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cal_arr_for_dcr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}